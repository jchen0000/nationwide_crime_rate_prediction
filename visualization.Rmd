---
title: "bm visualization"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(corrplot)
library(leaps)
library(car)
library(caret)
library(MASS) 
library(performance)
library(psych)
```
## Four assumptions associated with a linear regression:

1.Linearity: The relationship between X and the mean of Y is linear.

2.Homoscedasticity: The variance of residual is the same for any value of X.
                    We can test this assumption later, after fitting the linear model.

3.Independence: Observations are independent of each other.

4.Normality: For any fixed value of X, Y is normally distributed.

# Data import and clean

```{r}
data_df=read.csv("./data/cdi.csv") %>% 
  mutate(CRM_1000=crimes/pop*1000) %>%  ## add new variable CRM_1000(the crime rate per 1,000 population)
  dplyr::select(-crimes) %>% 
  dplyr::select(-id,-cty,-state) %>% 
  mutate(region=as.factor(region))

```

## TWO tables of 14  variables

```{r}
numeric_variable_tb=tibble(Variable=c("area","pop","pop18","pop65","docs","beds","hsgrad","bagrad","poverty","unemp","pcincome","totalinc","CRM_1000"),
                   mean=c(1041,393011,28.6,12.2,988,1459,77.6,21.1,8.72,6.60,18561,7869,57.3),
                   sd=c(1550,601987,4.19,3.99,1790,2289,7.02,7.65,4.66,2.34,4059,12884,27.3)) %>%knitr::kable(digit=2,caption="A Table for Numeric Variables")

factor_variable_tb=tibble(region=c("1 ( Northeast )","2 ( North central )","3 ( South )","4 ( West )"),
                          Frequency=c(103,108,152,77)) %>% knitr::kable(caption="A Frequency Table")
  
  



numeric_variable_tb
factor_variable_tb
## describe(data_df)
```
## distribution of target variable and log transformation for target variable.AT last,we generate a new variable log_CRM_1000 which is approximated to normal distribution
```{r}
hist(data_df$CRM_1000,xlab="CRM_1000",main="Distribution of Target Variable")  ## right-skewed so we use log transformation for target variable
data_df=data_df %>% 
  mutate(log_CRM_1000=log(CRM_1000)) %>% 
  dplyr::select(-CRM_1000)
hist(data_df$log_CRM_1000,,xlab="log_CRM_1000",main="Distribution of Target Variable")## bell-sharped so log transformation for target variable is perfect

```





## Checking outliers using boxplots

boxplot for each continuous variable

```{r}
par(mfrow=c(3,3))
boxplot(data_df$area,main='Land area measured in square miles')

boxplot(data_df$pop,main='Total population in 1990')

boxplot(data_df$pop18,main='Percent of population aged 18-34')

boxplot(data_df$pop65,main='Percent of populationß aged 65+')

boxplot(data_df$docs,main='Number of active physicians')

boxplot(data_df$beds,main='Number of hospital beds')


boxplot(data_df$hsgrad,main='Percent high school graduates')

boxplot(data_df$bagrad,main='Percent bachelor’s degrees')

boxplot(data_df$poverty,main='Percent of 1990 total population with income below poverty level')

par(mfrow=c(2,2))
boxplot(data_df$unemp,main='Percent below poverty level')

boxplot(data_df$pcincome,main='Per capita income')

boxplot(data_df$totalinc,main='Total personal income')

boxplot(data_df$log_CRM_1000,main='Log of the crime rate per 1,000 population')



```

## histogram(distribution of 12 independent numeric variables)

```{r}
data_without_region_df=
  data_df %>% dplyr::select(-region,-log_CRM_1000) ## since histogram cannot be plotted using a factor variable, we delete the variable region. 

par(mfrow=c(4,3))

for (n in 1:ncol(data_without_region_df)){
  var = names(data_without_region_df)[n]
  hist(data_without_region_df[,n],xlab = var,main=str_c("Distribution of"," variabe ",var))
}

```
from 12 histograms of each numeric variable, we notice the distribution of 5 histograms are right-skewed, therefore, we need to perform log transformation for these five variables(area,pop,docs,beds and totalinc)

## log transformation for five variables whose distribution is right-skewed 
```{r}
data_df = data_df %>% 
  mutate(log_area = log(area),
         log_pop = log(pop),
         log_docs = log(docs),
         log_beds = log(beds),
         log_totalinc = log(totalinc)
         ) %>% 
  dplyr::select(-pop,-area,-docs,-beds,-totalinc) 
```

## Re-check distribution of independent variables
```{r}
data_without_region_log_CRM1000_df=
  data_df %>% dplyr::select(-region,-log_CRM_1000)

## histogram after transformation
par(mfrow=c(4,3))
for (n in 1:ncol(data_without_region_log_CRM1000_df)){
  var = names(data_without_region_log_CRM1000_df[n])
  hist(data_without_region_log_CRM1000_df[,n],xlab = var,main=str_c("Distribution of"," variabe ",var))
}

```

After log transformation, in histograms, we notices except for the variable log_pop, right-skewed problems of four variables mentioned before has been solved.

## relationship of two variables(pairs)

```{r}
pair_df=
  data_df
    
pairs(data_df)

```

As shown in scatter plots,we can find it seems there is a linear relationship between any two of log_pop,log_docs,log_beds and log_totalinc

## Correlation plot

In theory,the correlation between independent variables should be zero.In particular,we expect and are okey with weak to no correlation between independent variables.
We also expect that independent variables reflect a high correlation with the target variable.

```{r}
cor_df=
  data_df %>%
  dplyr::select(-region) %>% 
  cor() ## Correlation coefficient

require(corrgram)
corrgram(data_df, order=TRUE)

corrplot(cor(cor_df), type = "upper", diag = FALSE)
```

from the correlaton plot,we can find independent variables like poverty(0.423),log_docs(0.423),log_beds(0.464) have a high correlation with the target variable log_CRM_1000.
In addition,we also notice independent variables like log_totalinc,log_beds,log_docs and log_pop have a high correlation with each other.

# Multicollinearity Assessment

Check collinearity. Stepwise variable selection
ResultL:remove four variables:pop,docs,bed and bagrad

A general rule of thumb for variance inflation factor(VIF):
A value of 1 indicates there is no correlation between a given predictor variable and any other predictor variables in the model.
A value between 1 and 5 indicates moderate correlation between a given predictor variable and other predictor variables in the model, but this is often not severe enough to require attention.
A value greater than 5 indicates potentially severe correlation between a given predictor variable and other predictor variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable.
```{r}
vif1 = lm(log_CRM_1000 ~ ., data = data_df)
summary(vif1)
vif(vif1) ##  log_totalinc has the highest VIF


vif2=lm(log_CRM_1000 ~.-log_totalinc, data = data_df)
summary(vif2)
vif(vif2) ## remove log_totalinc and log_pop has the highest VIF

vif3=lm(log_CRM_1000 ~.-log_totalinc -log_pop, data = data_df)
vif(vif3) ## remove log_pop and log_docs has the highest VIF

vif4=lm(log_CRM_1000 ~ .-log_totalinc -log_pop -log_docs , data = data_df)
vif(vif4) ## remove log_docs and bagrad has the highest VIF.

vif5=lm(log_CRM_1000 ~ .-log_totalinc -log_pop -log_docs -bagrad, data = data_df)
summary(vif5)
vif(vif5) ## remove bagrad  and remaining variables' VIFs are between 0 to 5 which indicate remaining variables have no multicollinearity. 



```

## Altogether, We remove four variables(log_totalinc,log_pop ,log_docs and bagrad ) whose VIFs are higher than 5
```{r}
data_df=data_df %>% 
        dplyr::select(-log_totalinc,-log_pop,-log_docs,-bagrad) 
```

The remaining 9 variables are pop18, pop65, hsgrad, poverty, unemp, pcincome, region, log_area, log_beds

## After log transformation and VIF test, we draw a new correlation plotfrom which we find all absoulte correlation coefficients are less than 0.7,indicating the absence of multicollinearity.
```{r}
cor_df=
  data_df %>%
  dplyr::select(-region) %>% 
  cor() ## Correlation coefficient


corrplot(cor(cor_df), type = "upper", diag = FALSE)
```


Above is revised  by Jie Liu. data_df has been dealt with well, use it directly.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

options(encoding='UTF-8')

library(tidyverse)
library(corrplot)
library(leaps)
library(car)
```

```{r}
data_df=read.csv("data/cdi.csv") %>% 
  mutate(CRM_1000=crimes/pop*1000) %>%  ## add new variable CRM_1000(the crime rate per 1,000 population) 
  select(-crimes,-pop) %>% ## Since new variable CRM_1000=crimes/pop*1000, we do not consider these two variables(crimes and pop) any more
  select(-id,-cty,-state)
  # mutate(region=as.factor(region))

data_df %>% skimr::skim_without_charts()
```


# histgram

```{r}
par(mfrow=c(4,4))
for (n in 1:ncol(data_df)){
  var = names(data_df)[n]
  hist(data_df[,n],xlab = var)
}

data_df = data_df %>% 
  mutate(log_area = log(area),
         log_docs = log(docs),
         log_beds = log(beds),
         log_totalinc = log(totalinc)) %>% 
  select(-area,-docs,-beds,-totalinc)
```



## each continuous virable distribution

boxplot for each continuous variable

```{r}
par(mfrow=c(3,4))
boxplot(data_df$area,main='Land area measured in square miles')


boxplot(data_df$pop18,main='Percent of population aged 18-34')

boxplot(data_df$pop65,main='Percent of populationß aged 65+')

boxplot(data_df$docs,main='Number of active physicians')

boxplot(data_df$beds,main='Number of hospital beds')


boxplot(data_df$hsgrad,main='Percent high school graduates')

boxplot(data_df$bagrad,main='Percent bachelor’s degrees')

boxplot(data_df$unemp,main='Percent below poverty level')

boxplot(data_df$pcincome,main='Per capita income')

boxplot(data_df$totalinc,main='Total personal income')

boxplot(data_df$CRM_1000,main='the crime rate per 1,000 population')

```


## relationship of two variables(pairs)
|
```{r}
pair_df=
  data_df %>% 
    select(-cty,-state) 
    
pairs(pair_df)

```
## Correlation plot

```{r}
cor_df=
  data_df %>% 
    select(-cty,-state,-region) %>% 
    cor() ## Correlation coefficient


corrplot(cor(cor_df), type = "upper", diag = FALSE)
```

相关系数:|r|<0.4为低度线性相关；0.4≤|r|<0.7为显著性相关；0.7≤|r|<1为高度线性相关

显著相关:
pop18 vs bagrad 0.45609703  
pop18 vs pop65  -0.616309639  
hsgrad vs poverty -0.691750483 
hsgrad vs unemp -0.593595788
hsgrad vs pcincome 0.52299613
bagrad vs unemp -0.540906913  
bagrad vs pcincome 0.69536186
poverty vs bagrad -0.40842385
unemp vs poverty 0.436947236 
pcincome vs poverty -0.60172504
CRM_100 VS poverty 0.471844218

高度线性相关:
bed vs docs 0.950464395 
hsgrad vs bagrad 0.70778672
totalinc vs docs 0.948110571  
totalinc vs beds 0.902061545

## Since correlation coefficient of bed and docs is the highest, draw a scatterpoint plot of these two variables

```{r}
data_df %>% 
  ggplot(aes(beds,docs)) + geom_point()

data_df %>% 
  ggplot(aes(totalinc,docs)) + geom_point()
```




## check if some variables approximate to normal distribution

```{r}
qqnorm(data_df$CRM_1000) #非常接近线性
qqnorm(data_df$area)
qqnorm(data_df$pop18) #接近线性
qqnorm(data_df$pop65)
qqnorm(data_df$docs)
qqnorm(data_df$beds)
qqnorm(data_df$hsgrad) #接近线性
qqnorm(data_df$bagrad) 
qqnorm(data_df$poverty) 
qqnorm(data_df$unemp) 
qqnorm(data_df$pcincome)  #接近线性
qqnorm(data_df$totalinc)
qqnorm(data_df$totalinc)
```
## establish a simple linear regression of CRM_1000 VS poverty

```{r}
slr1 = lm(CRM_1000~poverty,data=data_df)
summary(slr1)
anova(slr1)
plot(slr1)

```

## fit regression using all predictors(except ID,cty,state)

```{r}
data_df=data_df %>% 
  select(-cty,-state)
mult.fit = lm(CRM_1000 ~ ., data = data_df)
summary(mult.fit)
broom::tidy(mult.fit)

```
## 假设检验 正态性 qqplot
```{r}
qqPlot(mult.fit,id.method='identify',simulate = TRUE,main='Q-Q plot')
```
可以看到所有的点都在直线附近，并都落在置信区间内，这表明正态性假设符合得很完美

## 假设检验 独立性

进行D-W检验：
```{r}
durbinWatsonTest(mult.fit)
```
P = 0.124 > 0.05不显著，说明因变量之间无自相关性，互相独立。

## 假设检验 线性关系 成分残差图


```{r}
crPlots(mult.fit) 

```
成分残差图证实了线性假设，说明线性模型对该数据集是比较合适的。



## 同方差性

```{r}

mult.fit = lm(CRM_1000 ~ ., data = data_df)

ncvTest(mult.fit)
```
p = 2.037e-09 显著，说明误差方差不恒定。选用almost所有predictor的线性回归不满足同方差性
异方差性的出现意味着误差项的方差不恒定，这常常出现在有异常值（Outlier）的数据集上，如果使用标准的回归模型，这些异常值的重要性往往被高估。在这种情况下，标准差和置信区间不一定会变大还是变小。
